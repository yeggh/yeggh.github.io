# jemdoc: menu{MENU}{research.html}, showsource
= Research

*Multi-Sensor Data Fusion with Deep Learning in Image Processing Applications*

3D object detection has a significant impact on autonomous driving applications by reducing driving hazards and preventing harmful behaviors. Recently, the object detection task has been achieved using camera images or Light Detection and Ranging (LiDAR) sensors. While camera images provide high-resolution color and texture attributes, they are not prone to extreme weather and light conditions. Moreover, the point cloud output of the LiDAR sensor is not affected by illumination conditions, but it is hard to process this data since it is sparse, irregular, and deprived of characteristics offered by camera images.\n

In this project, we aim to provide a more comprehensive perception of the environment by integrating features from both LiDAR point cloud and camera images. The features extracted from 2D RGB images using Convolutional Neural Networks (CNNs) can make the 3D point cloud features richer using fusion-based methods. Also, many methods have been proposed for processing 3D data including 3D CNNs on volumetric represented data, processing 2D generated views of the 3D point cloud or processing the point represented data directly using PointNet. In this study, we explore Graph-based neural networks to maintain the 3D geometry (unlike methods on volumetric and 2D view representations) and process the sparse LiDAR data while taking locality into account (PointNet does not consider locality, and PointNet+\+ has an encoder-decoder network more suitable for dense data). Furthermore, the rich output of the above network will be fed to a 3D object detector, resulting in a high-accuracy 3D object detection.